# Code RAG System Configuration with Ollama
# 针对本地离线环境优化，使用Ollama作为LLM后端

# 项目设置
project:
  name: "my-cpp-project"
  root_path: "/mnt/d/Workspace/C++/pixel/Star_Vulkan/RenderCommandProject"  # D:\Workspace\C++\pixel\Star_Vulkan
  file_extensions:
    - ".cpp"
    - ".h"
    - ".hpp"
    - ".c"
    - ".cc"
  exclude_patterns:
    - "**/build/**"
    - "**/third_party/**"
    - "**/.git/**"
    - "**/test/**"

# 分块配置
chunking:
  strategy: "semantic"  # semantic | fixed | hybrid
  max_tokens: 1024
  overlap_tokens: 128
  include_context: true  # 包含文件路径、imports等

# Embedding配置
embedding:
  model_name: "BAAI/bge-large-zh-v1.5"  # 中文优化
  # model_name: "microsoft/codebert-base"  # 代码专用
  device: "cpu"  # 使用CPU进行embedding，避免GPU内存问题
  batch_size: 16
  normalize: true

# 向量数据库
vector_db:
  type: "chroma"
  persist_directory: "./data/chroma_db"
  collection_name: "code_chunks"

# 检索配置
retrieval:
  top_k: 10
  rerank: true
  rerank_model: "BAAI/bge-reranker-base"
  rerank_top_k: 5
  
  # 多路召回权重
  hybrid_search:
    enabled: true
    semantic_weight: 0.7
    keyword_weight: 0.3

# LLM配置 - 使用Ollama
llm:
  # Ollama本地部署
  backend: "ollama"  # 使用ollama后端
  api_base: "http://localhost:11434"  # Ollama默认地址
  model_name: "qwen2.5-coder:14b-instruct-q8_0"  # Ollama模型名称，确保已拉取此模型
  max_tokens: 4096  # 增加最大生成token数，充分利用Qwen2.5的长上下文能力
  temperature: 0.1
  
  # 上下文窗口管理
  max_context_tokens: 28000  # Qwen2.5支持32K上下文，设置为28K给输出预留空间
  reserved_output_tokens: 4096  # 为输出预留更多token

# 代码图谱（可选增强）
code_graph:
  enabled: true
  extract_calls: true
  extract_inheritance: true
  extract_includes: true